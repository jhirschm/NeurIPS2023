{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ee12453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob, os\n",
    "from util import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "052f7c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_arrays(*arrays):\n",
    "    return np.concatenate(arrays, axis=None)\n",
    "\n",
    "\n",
    "def re_im_concat(shg1, shg2, sfg):\n",
    "    vector = concatenate_arrays(np.real(shg1), np.real(shg2), np.real(sfg), np.imag(shg1), np.imag(shg2), np.imag(sfg))\n",
    "    return vector\n",
    "\n",
    "\n",
    "def split_and_concatenate(fields, scaling_factors, normalize_intensity=True, normalize_phase=True):\n",
    "    energy_out = np.array([])\n",
    "    intensity_out = np.array([])\n",
    "    phase_out = np.array([])\n",
    "    i = 0\n",
    "    for field in fields:\n",
    "        intensity = get_intensity(field)\n",
    "        phase = get_phase(field)\n",
    "        energy = calc_energy_expanded(field, scaling_factors[\"grid_spacing\"][i], scaling_factors[\"beam_area\"])\n",
    "\n",
    "        energy_out = concatenate_arrays(energy_out, energy)\n",
    "\n",
    "        int_factor = np.max(intensity)\n",
    "        if (int_factor == 0 or normalize_intensity==False):\n",
    "            intensity = intensity\n",
    "        else:\n",
    "            intensity = intensity / int_factor\n",
    "        intensity_out = concatenate_arrays(intensity_out, intensity)\n",
    "        \n",
    "        if normalize_phase:\n",
    "            phase_out = concatenate_arrays(phase_out, phase / np.pi)\n",
    "        phase_out = concatenate_arrays(phase_out, phase)\n",
    "        i += 1\n",
    "    return concatenate_arrays(intensity_out, phase_out, energy_out)\n",
    "\n",
    "def sample_concat1(directory, output_dir, scaling_factors, k=10, n=20, save=True):\n",
    "    '''\n",
    "    Concats .npy files in a given directory based on sliding length\n",
    "\n",
    "    Input:\n",
    "    directory: directory of samples\n",
    "    k: int, length of sliding window\n",
    "    n: int, number of files used to train\n",
    "\n",
    "    Output:\n",
    "    save new sample files in some new directory\n",
    "    '''\n",
    "    np.random.seed(42)\n",
    "    fn = os.path.join(directory, \"*.npy\")\n",
    "    files = glob.glob(fn)\n",
    "    np.random.shuffle(files)\n",
    "    i = -1\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    for split in np.array_split(files, n):\n",
    "        i += 1\n",
    "        print(i)\n",
    "        # get all dict\n",
    "        param_path = np.array([i.replace(\".npy\", \".pkl\") for i in split])\n",
    "        np.save(os.path.join(output_dir, f\"param_{i}.npy\"), param_path)\n",
    "        X = []\n",
    "        y = []\n",
    "        for f in split:\n",
    "            arr = np.load(f)\n",
    "\n",
    "            temp_array = np.array([split_and_concatenate([temp[:1892], temp[1892:1892 * 2], temp[-348:]],\n",
    "                                                         scaling_factors, unwrap_phase=False) for temp in arr])\n",
    "            np.save(os.path.join(output_dir, f\"temp_shape_{i}.npy\"), temp_array.shape)\n",
    "            arr = temp_array\n",
    "            scaler.partial_fit(arr.reshape(arr.shape[0], -1))\n",
    "            shape = (k, arr.shape[1])\n",
    "            y.append(arr[1:])\n",
    "            duplicate = np.repeat([arr[0]], k - 1, axis=0)\n",
    "            arr = np.concatenate((duplicate, arr[:-1]), axis=0)\n",
    "            new = np.array([[arr[i:i + k]] for i in range(len(arr) - k + 1)]).reshape(-1, k, arr.shape[1])\n",
    "            X.append(new)\n",
    "\n",
    "        X, y = np.concatenate(X), np.concatenate(y)\n",
    "\n",
    "        if save:\n",
    "            np.save(os.path.join(output_dir, f\"X_{i}.npy\"), X)\n",
    "            np.save(os.path.join(output_dir, f\"y_{i}.npy\"), y)\n",
    "            with open(os.path.join(output_dir, 'scaler.pkl'), 'wb') as file:\n",
    "                pickle.dump(scaler, file)\n",
    "\n",
    "    print(\"scaling new\")\n",
    "    for i in range(n):\n",
    "        X = np.load(os.path.join(output_dir, f\"X_{i}.npy\"))\n",
    "        y = np.load(os.path.join(output_dir, f\"y_{i}.npy\"))\n",
    "        X_new = np.copy(X)\n",
    "        y_new = scaler.transform(y.reshape(y.shape[0], -1))\n",
    "        for jj in range(X.shape[1]):\n",
    "            X_new[:, jj, :] = scaler.transform(X[:, jj, :].reshape(X.shape[0], -1))\n",
    "        if save:\n",
    "            np.save(os.path.join(output_dir, f\"X_new_{i}.npy\"), X_new)\n",
    "            np.save(os.path.join(output_dir, f\"y_new_{i}.npy\"), y_new)\n",
    "\n",
    "    return X, y, X_new, y_new, scaler, param_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24a7ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
