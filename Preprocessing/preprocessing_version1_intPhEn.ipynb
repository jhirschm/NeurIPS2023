{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c6ababc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob, os\n",
    "from util import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c7caabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_arrays(*arrays):\n",
    "    return np.concatenate(arrays, axis=None)\n",
    "\n",
    "\n",
    "def split_and_concatenate(fields, scaling_factors, normalize_intensity=True, normalize_phase=True):\n",
    "    energy_out = np.array([])\n",
    "    intensity_out = np.array([])\n",
    "    phase_out = np.array([])\n",
    "    i = 0\n",
    "    for field in fields:\n",
    "        intensity = get_intensity(field)\n",
    "        phase = get_phase(field)\n",
    "        energy = calc_energy_expanded(field, scaling_factors[\"grid_spacing\"][i], scaling_factors[\"beam_area\"])\n",
    "\n",
    "        energy_out = concatenate_arrays(energy_out, energy)\n",
    "\n",
    "        int_factor = np.max(intensity)\n",
    "        if (int_factor == 0 or normalize_intensity==False):\n",
    "            intensity = intensity\n",
    "        else:\n",
    "            intensity = intensity / int_factor\n",
    "        intensity_out = concatenate_arrays(intensity_out, intensity)\n",
    "        \n",
    "        if normalize_phase:\n",
    "            phase_out = concatenate_arrays(phase_out, phase / np.pi)\n",
    "        else:\n",
    "            phase_out = concatenate_arrays(phase_out, phase)\n",
    "        i += 1\n",
    "    return concatenate_arrays(intensity_out, phase_out, energy_out)\n",
    "\n",
    "def sample_concat1(directory, output_dir, scaling_factors, k=10, n=20, save=True, normalize_intensity=True, normalize_phase=True, stop_early=False, stop_early_num = 1):\n",
    "    '''\n",
    "    Concats .npy files in a given directory based on sliding length\n",
    "\n",
    "    Input:\n",
    "    directory: directory of samples\n",
    "    k: int, length of sliding window\n",
    "    n: int, number of files used to train\n",
    "\n",
    "    Output:\n",
    "    save new sample files in some new directory\n",
    "    '''\n",
    "    np.random.seed(42)\n",
    "    fn = os.path.join(directory, \"*.npy\")\n",
    "    files = glob.glob(fn)\n",
    "    np.random.shuffle(files)\n",
    "    i = -1\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    for split in np.array_split(files, n):\n",
    "        i += 1\n",
    "        print(i)\n",
    "        # get all dict\n",
    "        param_path = np.array([i.replace(\".npy\", \".pkl\") for i in split])\n",
    "        np.save(os.path.join(output_dir, f\"param_{i}.npy\"), param_path)\n",
    "        X = []\n",
    "        y = []\n",
    "        for f in split:\n",
    "            arr = np.load(f)\n",
    "\n",
    "            temp_array = np.array([split_and_concatenate([temp[:1892], temp[1892:1892 * 2], temp[-348:]],\n",
    "                                                         scaling_factors,normalize_intensity=True, normalize_phase=True) for temp in arr])\n",
    "            np.save(os.path.join(output_dir, f\"temp_shape_{i}.npy\"), temp_array.shape)\n",
    "            arr = temp_array\n",
    "            scaler.partial_fit(arr.reshape(arr.shape[0], -1))\n",
    "            shape = (k, arr.shape[1])\n",
    "            y.append(arr[1:])\n",
    "            duplicate = np.repeat([arr[0]], k - 1, axis=0)\n",
    "            arr = np.concatenate((duplicate, arr[:-1]), axis=0)\n",
    "            new = np.array([[arr[i:i + k]] for i in range(len(arr) - k + 1)]).reshape(-1, k, arr.shape[1])\n",
    "            X.append(new)\n",
    "\n",
    "        X, y = np.concatenate(X), np.concatenate(y)\n",
    "\n",
    "        if save:\n",
    "            np.save(os.path.join(output_dir, f\"X_{i}.npy\"), X)\n",
    "            np.save(os.path.join(output_dir, f\"y_{i}.npy\"), y)\n",
    "            with open(os.path.join(output_dir, 'scaler.pkl'), 'wb') as file:\n",
    "                pickle.dump(scaler, file)\n",
    "                \n",
    "        if (stop_early and i==stop_early_num):\n",
    "            break\n",
    "\n",
    "    print(\"scaling new\")\n",
    "    for i in range(n):\n",
    "        X = np.load(os.path.join(output_dir, f\"X_{i}.npy\"))\n",
    "        y = np.load(os.path.join(output_dir, f\"y_{i}.npy\"))\n",
    "        X_new = np.copy(X)\n",
    "        y_new = scaler.transform(y.reshape(y.shape[0], -1))\n",
    "        for jj in range(X.shape[1]):\n",
    "            X_new[:, jj, :] = scaler.transform(X[:, jj, :].reshape(X.shape[0], -1))\n",
    "        if save:\n",
    "            np.save(os.path.join(output_dir, f\"X_new_{i}.npy\"), X_new)\n",
    "            np.save(os.path.join(output_dir, f\"y_new_{i}.npy\"), y_new)\n",
    "        \n",
    "        if (stop_early and i==stop_early_num):\n",
    "            break\n",
    "\n",
    "    return X, y, X_new, y_new, scaler, param_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b73f783c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required data and set up vectors and dictionaries (paths may needed to be adjusted based on your operating system, file structure, and from where code is being run)\n",
    "freq_vectors_shg1 = np.load(\n",
    "    \"../Data/shg_freq_domain_ds.npy\")\n",
    "freq_vectors_shg2 = freq_vectors_shg1 # these are equivalent here\n",
    "freq_vectors_sfg = np.load(\n",
    "    \"../Data/sfg_freq_domain_ds.npy\")\n",
    "\n",
    "domain_spacing_1 = (freq_vectors_shg1[1] - freq_vectors_shg1[0]) * 1e12 #scaled to be back in Hz\n",
    "domain_spacing_2 = (freq_vectors_shg2[1] - freq_vectors_shg2[0]) * 1e12\n",
    "domain_spacing_3 = (freq_vectors_sfg[1] - freq_vectors_sfg[0]) * 1e12\n",
    "\n",
    "factors_freq = {\"beam_area\": 400e-6 ** 2 * np.pi,\n",
    "                \"grid_spacing\": [domain_spacing_1, domain_spacing_2, domain_spacing_3],\n",
    "                \"domain_spacing_1\": domain_spacing_1, \"domain_spacing_2\": domain_spacing_2,\n",
    "                \"domain_spacing_3\": domain_spacing_3} #beam radius 400 um (and circular beam)\n",
    "\n",
    "data_directory = \"/sdf/group/lcls/ds/scratch/s2e_scratch/Data/SFG_0504\" #this is where you downloaded data from SDR repo\n",
    "output_dir = \"/sdf/group/lcls/ds/scratch/s2e_scratch/Data/SFG_intPhEn/test\" #this is where you want to store the reformatted data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "980f97eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "scaling new\n"
     ]
    }
   ],
   "source": [
    "X1, y1, X1_new, y1_new, scaler1, file1 = sample_concat1(data_directory, output_dir, scaling_factors=factors_freq, k=10, n=100, save=True, stop_early=False, stop_early_num=3)\n",
    "with open(os.path.join(output_dir, 'scaler_bckkup.pkl'), 'wb') as file:\n",
    "    pickle.dump(scaler1, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e4daa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
